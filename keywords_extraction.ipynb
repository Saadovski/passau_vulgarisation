{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('kwp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "21fa1f2444d3e1da2872dc64bba334c5ef541086d481e62347d853abdff2f080"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc ='''In this paper we propose a novel self-supervised\n",
    "approach of keywords and keyphrases retrieval and extraction\n",
    "by an end-to-end deep learning approach, which is trained by\n",
    "contextually self-labelled corpus. Our proposed approach is\n",
    "novel to use contextual and semantic features to extract the\n",
    "keywords and has outperformed the state of the art. Through\n",
    "the experiment the proposed approach has been proved to be\n",
    "better in both semantic meaning and quality than the existing\n",
    "popular algorithms of keyword extraction. In addition, we\n",
    "propose to use contextual features from bidirectional\n",
    "transformers to automatically label short-sentence corpus with\n",
    "keywords and keyphrases to build the ground truth. This\n",
    "process avoids the human time to label the keywords and do not\n",
    "need any prior knowledge. To the best of our knowledge, our\n",
    "published dataset in this paper is a fine domain-independent\n",
    "corpus of short sentences with labelled keywords and\n",
    "keyphrases in the NLP community.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (1, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['propose novel self', 'we propose novel', 'keyword extraction', 'algorithms of keyword', 'of keyword extraction']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['popular algorithms of',\n",
       " 'keyphrases retrieval',\n",
       " 'propose novel',\n",
       " 'existing popular algorithms',\n",
       " 'we propose novel']"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=13) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['of keyword extraction',\n",
       " 'existing popular algorithms',\n",
       " 'we propose novel',\n",
       " 'outperformed the state',\n",
       " 'deep learning approach']"
      ]
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = '''\n",
    "- L'algo récupère les mots-clés avec plus ou moins de succès en fonction du document, du nombre de keywords qu'on cherche (1-3), du paramètre indicant la diversité dans les algos mmr et max_sum_min, et de si les stop-words sont gardés ou pas.\n",
    "- Les paramètres nr_candidates = 13 et diversity = 0.4 respectivement pour max_sum_min et mmr sont un bon compromis pour equilibrer entre la variété des mots-clés et leur pertinence.\n",
    "- En général les mots-clés que BERT extrait ne correspondent pas toujours aux mots les plus pertinents decrivant le mieux le document, mais sont souvent aussi des mots-clés qui pourraient demander une definition ou une explication si le lecteur n'es pas familiarisé avec le domaine sur lequel traite le document\n",
    "\n",
    "'''"
   ]
  }
 ]
}